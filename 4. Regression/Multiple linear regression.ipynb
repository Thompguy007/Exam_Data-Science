{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44682b79-7daa-46e6-944a-40849e8229ef",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "In this notebook, we will see how to do multiple linear regression in Python using statsmodels and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae20424-cc49-4d65-9036-bb0cde949319",
   "metadata": {},
   "source": [
    "First we import the standard packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46c6aa-4b4f-4aa4-9cc4-a48b76ece40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d31d9-1702-472c-a140-c6724bb125d6",
   "metadata": {},
   "source": [
    "As example data, we will use the Ames housing data, see kaggle: https://www.kaggle.com/datasets/marcopale/housing?resource=download. It is also available on Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd1835-4882-4b70-9a90-b85232d43d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv(\"AmesHousing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb81017-7f92-4697-819d-20cfb54c7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d6d06-194e-488d-ac7a-b47201ecabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd673298-0443-4841-b761-fa74a9f80b61",
   "metadata": {},
   "source": [
    "As we now have more than one predictor/feature/independent variable, we cannot visualize the linear relationship anymore. thus we will skip the plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a36a4-c6a7-4cd0-823d-fb48107c940a",
   "metadata": {},
   "source": [
    "Now, however, we have a more complex dataset and we need to do a bit of preprocessing before we can use it for multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d04fc-d23c-4352-9022-621846485de6",
   "metadata": {},
   "source": [
    "For simplicity we will only use the following predictor variable: \n",
    "\n",
    "- `Lot Area` (the size of the lot in square feet)\n",
    "- `Overall Cond` (rating of the overall condition of the house)\n",
    "- `Year Built` (Original construction year)\n",
    "- `Gr Liv Area` (Above grade (ground) living area in square feet)\n",
    "- `TotRms AbvGrd`(Total rooms above grade, excluding bathrooms)\n",
    "- `Mo Sold` (Month Sold)\n",
    "- `Yr Sold` (Year Sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042437b-746c-494d-ad7c-52f3df0b3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames = ames[[\"Lot Area\", \"Overall Cond\", \"Year Built\", \"Gr Liv Area\", \"TotRms AbvGrd\", \"Mo Sold\", \"Yr Sold\"]]\n",
    "X_ames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c23681-2b76-4c64-822e-958ec9002b4a",
   "metadata": {},
   "source": [
    "For the response/target/dependent variable, we use `SalePrice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47ace3-10f1-4406-88c7-721c9be3996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ames[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa93b3-c217-4f1d-9a1d-e9ef8c50ebe8",
   "metadata": {},
   "source": [
    "## Fitting a multiple linear regression model using OLS and statsmodels\n",
    "\n",
    "We can fit a multiple regression model to our data using OLS just as for simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb4719-4103-449c-9a3e-c8793d06a18f",
   "metadata": {},
   "source": [
    "We add an intercept to X as in the case of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe622d-beb7-4b13-b19a-728bc0becaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames_wInt = sm.add_constant(X_ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b7a2b-c152-4bc8-ac5c-9b727f6d4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames_wInt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7b666-f7f4-4dac-8708-e02485864242",
   "metadata": {},
   "source": [
    "We can now with a multiple linear regression model using OLS just as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae82d4-f388-44f6-9273-99e7ca94a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model = sm.OLS(y, X_ames_wInt).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875d2a6-16ba-44a4-a703-c26f909f2724",
   "metadata": {},
   "source": [
    "We can get a lot of information about our model from the summary method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83556a9-77e9-44de-9d70-c27757f8a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a21d3d-65c6-4d0c-99c9-257837289cbe",
   "metadata": {},
   "source": [
    "**Interpretation of the model**\n",
    "\n",
    "We see a fairly high R-squared of 0.688.\n",
    "\n",
    "The intercept, month sold, and year sold are not statistically significantly different from 0, the rest of the predictor variables are. Depending on the purpose of our modeling, it might make sense to remove the variables which coefficients are not significantly different from zero (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b60d9-ed05-42c1-8a09-b0c067f21130",
   "metadata": {},
   "source": [
    "### Retrieving coefficients and plotting fitted regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de33ae-0cf3-4700-b62f-e59fa6cd6c57",
   "metadata": {},
   "source": [
    "We can also get the parameters or coefficients from the fitted model the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0470d78-2937-4ccd-98e8-04f3b697fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5784f5b-d4a0-418e-85ab-46417b999310",
   "metadata": {},
   "source": [
    "We can get the predictions as before using the predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d10f69-044a-4b45-b93e-c900830232de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = mulinreg_model.predict()\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1640be-6df3-41a7-a1cd-a22d963a39e9",
   "metadata": {},
   "source": [
    "### Model performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff044b00-4353-4fc8-b2fe-0bf65e859c4e",
   "metadata": {},
   "source": [
    "We can also get the R-squared directly from the fitted model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857be40-6285-498d-a61c-ee17e8483826",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda14bc-6722-4419-b1f4-9f8fde0c3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.rsquared_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afda5c-b124-4569-b498-1ce09f1a64d1",
   "metadata": {},
   "source": [
    "We can get the residuals of the model also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16449e-8361-4397-bd38-5768eeac803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03182a8-77f3-49e7-8327-1d46728b0322",
   "metadata": {},
   "source": [
    "Using these, we can calculate the *MAE* (Mean Absolute Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f8586-f3d2-4f74-a57c-6d997d63b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(mulinreg_model.resid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d13a4c-2f9b-46ac-a7cb-b115ae19b0e4",
   "metadata": {},
   "source": [
    "and the *MSE* (Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26878f-1f26-4e22-b3ef-7ad0c533756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mulinreg_model.resid**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a54e8-4adb-4963-8022-1e442fdce9a3",
   "metadata": {},
   "source": [
    "and the *RMSE* (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac74a29-4d3c-40c1-a83c-b614b1bd1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(mulinreg_model.resid**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab23f15-4ffd-487f-8cb2-ae96a800c5f6",
   "metadata": {},
   "source": [
    "## Fitting multiple linear regression using scikit-learn\n",
    "\n",
    "We can also use the scikit-learn package to fit a multiple regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a0725-5baf-4944-b20d-198ecf240253",
   "metadata": {},
   "source": [
    "Then we define a linear regression mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a9697-629d-4b72-80e6-f78d24ce9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87538182-62ee-4091-b908-a0ffbbaaf7a1",
   "metadata": {},
   "source": [
    "We can now fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf1d8d5-123c-42cb-9bed-95eb872f3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk.fit(X_ames, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0c7c6-41af-4f13-902a-a7571fb1d64b",
   "metadata": {},
   "source": [
    "Getting the coefficient in scikit-learn will give us all the coefficient except the coefficient for the intercept. That we have to get seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47867c-2ce4-4c53-9b65-74801615fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844f05b-1aa5-4c7f-ac92-f819b28e2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c96d9-2031-4dc3-8f9f-14f5b6a69305",
   "metadata": {},
   "source": [
    "We see that we get the same value as for statsmodels OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d522fe6-ee19-440a-bc2c-d8281ec018b7",
   "metadata": {},
   "source": [
    "To get the prediction of the model, we can use the `.predict` method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18ff7f-4e84-44bb-a174-ff388527ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk.predict(X_ames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104fa46-569e-4484-a082-95ae1c383b5f",
   "metadata": {},
   "source": [
    "We can use the metrics submodule from scikit-learn again to calculate the performance of the multiple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44e001-5577-4978-9273-e3dd33ef8442",
   "metadata": {},
   "source": [
    "All of these take the true and the predicted values as arrays. Thus, we first define a variable for the predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ccfa9-9c03-4a2a-a8e3-5692c090a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sk = mulinreg_model_sk.predict(X_ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7aca7d-e7f5-4bac-b600-2b863b088d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1368e75-8cd7-4213-af4f-065ef71c8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y, y_pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa8ce2-c76b-433f-8ea9-c67181733995",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, y_pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34676f7-c06a-4a1b-9ce5-048b1237ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y, y_pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20434b3f-5c0d-4ec5-8d0a-669f005a7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad97204-a8c4-4a70-8fe0-ce34ab196e7e",
   "metadata": {},
   "source": [
    "## Dealing with categorical variables\n",
    "\n",
    "We will use the categorical variable `Bldg Type` (Type of dwelling) from the ames dataset for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2f961-ca7b-4eb2-a3fb-5404f95b4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ames[[\"Bldg Type\"]].groupby(ames[\"Bldg Type\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7d4d7-7584-4551-8426-7df4df16db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(ames[\"Bldg Type\"], drop_first=True, dtype = \"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b9323-d119-407e-89b9-433d0fc53483",
   "metadata": {},
   "source": [
    "We use the `get_dummies`from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a78a9-0306-4ca1-8e2b-3bda45533f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames_wInt = X_ames_wInt.join(pd.get_dummies(ames[\"Bldg Type\"], drop_first=True, dtype = \"int\"))\n",
    "\n",
    "X_ames = X_ames.join(pd.get_dummies(ames[\"Bldg Type\"], drop_first=True, dtype = \"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9b34f-55ce-4bf5-bbb0-8b2aea5719ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames_wInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242c89b-e1ab-4563-8a63-c3a4df82200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031bf16-d566-42a8-b1e7-8efcd15387f5",
   "metadata": {},
   "source": [
    "Note that it is the *1Fam* building type that is left out, thus it will be the reference type used in interpreting the regression coefficent for this categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16125741-6a90-44cf-b921-111eb921ab35",
   "metadata": {},
   "source": [
    "We can now retrain our multiple regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54e0d9-8f29-4904-9561-f6145b3824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model = sm.OLS(y, X_ames_wInt).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36360e4-66e9-416c-b21d-f43686aa98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22220f-9453-4af5-91dc-db65d95e7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model.params[\"2fmCon\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01051a3-b1a3-45df-b3c2-808914e154c8",
   "metadata": {},
   "source": [
    "**Interpretation of the model**\n",
    "\n",
    "We can see that we increased the R-squared of the model, and several of the building types are significant. As we are dealing with multiple regression, we should actually look at adjusted R-squared instead of R-squared. But Adjusted R-squared has also improved.\n",
    "\n",
    "That the coefficient for 2fmCon is -12976 (-1.298e+04) means that the selling price for a 2fmCon building is 12976 less than a 1Fam building, everying else being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdd08a-29b0-4ed2-b6d8-24a1e0341ee1",
   "metadata": {},
   "source": [
    "Calculating performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09eead8-87cf-4cd5-b27e-78f81547ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = mulinreg_model.predict()\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02b47b-ea4c-4a0b-8db9-c497c7cb1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab396f-3f04-4059-888c-920704cbaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa50eb-4ddb-4644-90d1-f36eefe2ad18",
   "metadata": {},
   "source": [
    "We can see that we do get a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15221543-4f7b-458d-bc69-e41c319982c9",
   "metadata": {},
   "source": [
    "Training the model with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ceb11e-3e05-4d0f-b57a-f4664a899a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823359d-894f-4c0c-95b8-c09496306da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_sk.fit(X_ames, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7d0df-31f8-4b6e-85f5-27d4ea2dde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sk = mulinreg_model_sk.predict(X_ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69516b48-2eb3-4a29-832d-ebb59fac827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_pred_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac8514-ea23-4b51-a8a1-92c6c67fbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_squared_error(y, y_pred_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0882785b-904f-424d-9505-4adcd1bbde37",
   "metadata": {},
   "source": [
    "## Looking at assumptions and problems\n",
    "\n",
    "We will now look at the assumptions and potential problems for our multiple linear regression model.\n",
    "\n",
    "If these assumptions are not meet, it means that we cannot completely trust the statistical calculations such as the p-values. Moreover, our evaluation metrics might not truly represent the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd3f89-dfc3-4ff7-a32b-a659be9a32fa",
   "metadata": {},
   "source": [
    "### Non-linearity of the data\n",
    "\n",
    "One of the assumptions of linear regression is that there is a linear relationship between the independent (X) variables and the dependent variable (y). \n",
    "\n",
    "For simple linear regression, we can make a scatterplot of the x and y variable and visually inspect for linear relationship. However, for multiple linear regression, we cannot do that. Instead, we can plot the residuals versus the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41650eb-3ab6-4c07-8b41-10719921c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = pred_y, y = mulinreg_model.resid)\n",
    "plt.plot(pred_y, np.repeat(0, len(pred_y)), color = \"orange\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc9a70-02a2-4bcf-9da5-c1dbba831ce5",
   "metadata": {},
   "source": [
    "They look like they go up a bit, maybe the relationship is not quite linear, but the points almost fall equally along the x-axis. In other words, there is no clear pattern suggesting that the data is not approximately linear."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "490823c6-c48e-465a-a880-c20fb3233ddf",
   "metadata": {},
   "source": [
    "### Correlation of error terms \n",
    "\n",
    "Another assumption of linear regression is that the error terms are uncorrelated, in other words, the i'th error term $e_i$ does not tell us anything about i+1'th error term $e_{i+1}$. Thus, to investigate this, we can plot the residuals in order of their appearance (or by time, if there is a time variable). As we have no time variable, we can instead plot the residuals versus their row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39c053-ad7b-45d7-a35a-6820b6cf7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(12, 8))\n",
    "sns.lineplot(x = range(0,len(mulinreg_model.resid)), y = mulinreg_model.resid)\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75c273-6cfe-44f0-852a-e9a60ce2951b",
   "metadata": {},
   "source": [
    "It is actually a bit hard to see from this plot if there is any correlation between the error terms, in terms of longer sections of value above or below 0. However, there are no clear such sections or pattern, so there is no obvious correlation, at least. Thinking about the data, it is also not obvious how a correlation among errors should have arisen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0945c-d526-498a-afd2-dba74800b893",
   "metadata": {},
   "source": [
    "### Non-constant variance of error terms\n",
    "\n",
    "Another assumption of linear regression is that the error terms have constant variance. One common way of seeing non-constant error terms it to look at the plot of residuals versus predicted values a look whether the variance increase as the predicted values do - it will look like a funnel. So let us look at this plot again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05491f13-83e1-4b92-b5b4-fb4c5ccafd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(8, 5))\n",
    "sns.scatterplot(x = pred_y, y = mulinreg_model.resid)\n",
    "plt.plot(pred_y, np.repeat(0, len(pred_y)), color = \"orange\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556e988-7894-488c-9b02-5c8d27efbab9",
   "metadata": {},
   "source": [
    "It indeed looks like there is a bit of a funnel, that is, an increase in variance. Thus, the assumption of constant variance of the error terms might indeed be violated.\n",
    "\n",
    " One possible solution would be to transform the dependent/target variable $y$ to $log(y)$ instead. This will potentially work in this case, but it will also make the interpretation of the model harder (the coefficient will mean something else) and we need to transform each prediction as well to get back the price of the house. We will not do this not, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72774b-8f11-48f7-a6bc-8f905fc859ed",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outliers in regard to linear regression models, are points for which the predicted value is very far from the actual values. These we can also spot in the residual vs predicted values plot. So let us not make the plot again, but just look at the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae57fb0-3ae8-409f-b29b-d2bce607c684",
   "metadata": {},
   "source": [
    "The three points, at the bottom right, looks like they could be outliers as their absolute residual values are quite high. Thus, it would make sense to investigate those point further to see if there are any errors in the data or other reasons from them being outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3734f04-6d84-4c71-82b1-b6cf52ac7c18",
   "metadata": {},
   "source": [
    "### High leverage points\n",
    "\n",
    "High leverage points are a bit like residuals. They are point that as a high influence on how the model looks. In simple linear regression, this means that a high leverage point might heavily affect where the regression line lies. Note that outliers are extreme values in a sense, but they do not need to have high leverage as they might not affect the actual model fit that much.\n",
    "\n",
    "High leverage points are usually points that have an x-value far away from the other x-values in the dataset. This is easy to spot in simple linear regression where we only have on x variable. However, it is much harder to spot directly for multiple linear regression. Luckily, there is a leverage statistics we can calculate for each point based on a linear regression model, which we can use to make a \"Leverage plot\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e6a3e-7592-4d70-9078-908c8282bbcb",
   "metadata": {},
   "source": [
    "We can calculate the leverage statistics for each point in the following for statsmodels (does not work for scikit-learn models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbca153-88d0-41c0-94ad-be3506fad939",
   "metadata": {},
   "outputs": [],
   "source": [
    "leverageStats = mulinreg_model.get_influence().hat_matrix_diag\n",
    "leverageStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a71c38-54e1-41bd-815e-cf70d4ae4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", figsize=(12, 8))\n",
    "sns.scatterplot(x = range(0, len(leverageStats)), y = leverageStats)\n",
    "plt.ylabel(\"Leverage\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a7339-8125-46bd-b99b-53a348f3aaa9",
   "metadata": {},
   "source": [
    "Here there are clearly 3(/4) point that stands out with much higher leverage than the other points. Thus, it could again be useful to investigate these points further. (We will not do that now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c3c5f-1a51-46e5-ada6-2f82fa42501e",
   "metadata": {},
   "source": [
    "### Collinearity\n",
    "\n",
    "Collinearity can also be a problem for linear regression models and refers to the existence of high correlation between two or more of the independent/predictor variables. IF two predictor variables are highly correlated, it can be hard/impossible for a linear regression model to separate out the effect on the response variable, coming from each of them - one might have a really high positive effect, while the other might have a really high negative effect, but in reality neither of them might have a big impact on the response variable.\n",
    "\n",
    "The easiest way to spot collinearity between any pair of variables is to look at the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d3bc0-0b5a-447a-8461-0b4ebfc19e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930752d6-2be1-4ba4-94bd-79c3a7e1f910",
   "metadata": {},
   "source": [
    "We do not see any large correlation between the x variables here that could be a problem. The only correlation that is a bit (to?!) high is the one between `TotRms AbvGrd` and `Gr Liv Area`. The easiest way to deal with collinearity among the predictor variables is simply to drop one of them. Usually this well not make the model much worse as on the of the variable contains most of the information contained in the other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52ef81-e4a4-40af-b00d-b8e44bbbf644",
   "metadata": {},
   "source": [
    "It is worth noticing that we could potentially have collinearity between a set of more than two variables. this could create problems for the linear regression model, but it is not always possible to spot this from the pairwise correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea30b76-101e-4bdd-b405-662b8725f31b",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "As we mentioned previously, it might sometimes make sense to remove variables whose coefficient is not significantly different from zero. It does not make sense to remove some of the dummy variables created from the same variable, so we would not remove `TwnhsE` even though it has a very high p-value. However, we might consider removing the intercept (`const`), `Mo Sold`, and `Yr Sold`.\n",
    "\n",
    "If we just try to achieve the highest predictive accuracy (RMSE for instance), it makes good sense to try without those variables. If are interested in answering the more inferential question of what affect the sales price of a house, it also makes sense to remove the non-significant variables. However, in some cases we might want to keep the variable if we are interested in those particular, that is if we are interested in the effect of the sales data on the price of the house. \n",
    "\n",
    "Let us say we are interested in predictive performance (measured by RMSE) and in achieving a high adjusted R-square, and let see if we can improve those by removing non-significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302653f4-51fd-4972-8887-62aec4c7f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ames_onlySig = X_ames_wInt.drop(columns=[\"const\", \"Mo Sold\", \"Yr Sold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a0947-6946-4ba3-ad0f-753219e644cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_onlySig = sm.OLS(y, X_ames_onlySig).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff30ed-7eae-40a0-8298-9c46558badae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mulinreg_model_onlySig.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acafb11-6da2-42cf-bef7-a62981b389ff",
   "metadata": {},
   "source": [
    "Now we clearly improved the adjusted R-square from $0.703$ to $0.928$, which is quite of an improvement! Let us see if we also improved RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd84d8d-72d2-41b7-8879-f88d1e74cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_onlySig = mulinreg_model_onlySig.predict()\n",
    "root_mean_squared_error(y, pred_y_onlySig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386590f8-d3fe-41c3-9c1d-cc2892884f1b",
   "metadata": {},
   "source": [
    "Now, for predictive performance the model actually got worse, in the sense that RMSE increased substantially from $43473$ to $53070$. Leaving out variables never improve predictive performance (measured by RMSE) for linear regression when we measure RMSE on the same data as we trained on. However, as we will talk about next time, we should never measure predictive performance on the same data as we trained the model. If we measure RMSE on data the model has not seen before, we might improve performance be removing some insignificant variables - this is especially true for other model types than linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cdec1-9cb9-4a07-9bed-d55f9dcfb07c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
